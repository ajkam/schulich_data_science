{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objective for Data Analysis: I will be assuming the role of a data analyst in an ecommerce company that operates overall multiple regions with multiple products skews. The goal for this data analysis is to understand more about the sales performance of various products."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will hope to analyze the sales performance on two levels. Firstly the VP of sales level and secondly the regional sales manager level. I am assuming that there is a VP of sales that oversees the sales of all regions and then regional sales managers that are responsible for the performance a particular region. This hierachal breakdown is typical in most sales department and as a result the data analysis will focus on answering different questions for each department."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the VP of sales I plan to answer to following questions by extracting observations and the providing recommendations for the business:\n",
    "\n",
    "Firstly I want to garner information regarding: Which regions are performing well and not well? \n",
    "This will lead to recommendations which regions need more sales training because of poor performance and which regions need to be rewarded because they are performing well. As a good VP of sales should use incentives and resources to help motivate their employees.\n",
    "\n",
    "Secondly I want to garner infomation regarding: Which products are selling well and not well on a unit basis vs a price basis? \n",
    "\n",
    "This will lead to recommendations on to which products should be made available in more regions because they have a higher sales revenue and which products should be removed from regions or discontinued because they are lower sales revenue.\n",
    "\n",
    "Thirdly I want to ensure the first two observations are mutually exclusive and collectively exhaustive. I will do this by diving deeper into the analysis by answering the following four questions:\n",
    "\n",
    "Of the regions not performing well are they just doing poorly or are they only selling low-performing products?\n",
    "\n",
    "Of the regions performing well, are they doing well or are they only selling high-performing products?\n",
    "\n",
    "Of the products not performing well are they just doing poorly or are they only selling low-performing regions?\n",
    "\n",
    "Of the products performing well are they just doing well or are they only selling high-performing regions?\n",
    "\n",
    "Note that all this inofrmation will observed over time of the three years of data included from 2022 to 2024 to observe trends in sales performance from products and regions over time.\n",
    "\n",
    "This will allow the VP of sales be more specific in the recommendations for products and regions by truly isolating the factors that contriubting to higher sales performance.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the managers I aim to answer the following questions by providing insights into the specific performance of their region and provide recommendations on how to improve their performance.\n",
    "\n",
    "Firstly taking from the VP of Sales Analysis I will answer the question: Where do I rank in comparison to my peers regarding overall sales performance over time?\n",
    "\n",
    "This will provide them an accurate information for when and for which products their sales strategy is succeeding.\n",
    "\n",
    "Secondly to follow up on the previous observations I can answer the following questions to become more granular:\n",
    "\n",
    "Which products are doing well in my region versus other people’s regions? \n",
    "Which products are doing worse in my region versus other people’s regions?\n",
    "\n",
    "This will allow them to understand what regional sales manager they need to consult in order to improve the sales strategy for different products. This will also allow them to rate how effective their successful sales strategies really are.\n",
    "\n",
    "Thirdly I will firmly answer based on sales performance: What products should I start selling in my region and what products should I stop selling?\n",
    "\n",
    "Although this might be answered in the other questions it is still good to plot it out for each manager.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above analysis will help the sales managers and sales VP come up with an efficient strategy for the 2025 sales year. Before I can begin answering those questions, visualizing observations, and recommending actions I will begin by cleaning the dataset. The idea with cleaning this dataset is to ensure that someone coming into this code for the first time will be able to understand exactly what I did. I would also like the ensure that if they need to make modifications to my analysis due to new information on values or rerun the same analysis for themselves they will have the means to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is necessary to import pandas, seaborn, and numpy which are the packages required for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ajkam/schulich_data_science/main/Assignments/sales_data%20(1)%20-%20Copy.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is used to read the excel file store on github and store into a dataframe titled 'df'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display info on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe some statistics regarding the datrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that an assumption was made that the Price column represented price per unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict = {'Product':'product_sku', 'Region':'region','Price':'price_per_unit', \n",
    "               'Units Sold':'units_sold','Sale Date':'sale_date' }\n",
    "df= df.rename(columns=column_dict)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create a list of  the old column headings and new easier to work with headings\n",
    "use the rename function to replace the column headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the isna function to check if their any missing values in each column and sum the total number of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use the boolean function .any to check if the dataframe columns has any missing values so its easy to compare with down the line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the data set with the boolean function isnull to see true and false for where data is missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram for the units_sold column to see how it is distributed. Based on the output it seems that is normally distributed with no skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_sold_mean = df['units_sold'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is normally distriubted with no skewness I have determined the best approach is to fill the missing data with the mean. So this function uses the .mean() function to get the mean of units sold and store the value in units_sold_mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['units_sold'].fillna(value=units_sold_mean, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the fillna function to replace empty values in units_sold with the units_sold_mean value. The inplace argument is left as True so the change is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dispaly the histogram again to view the change as a new column that is over a count of 200 is a direct result of filling in the missing values we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the describe function to check if the mean has changed and it has not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram for the price_per_unit column to see how it is distributed. Based on the output it seems that the data skewed to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_per_unit_median = df['price_per_unit'].median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there is skewness to the left I have determined the best approach is to fill the missing data with the median. So this function uses the .median() function to get the median of price_per_unit and store the value in price_per_unit_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_per_unit'].fillna(value=price_per_unit_median, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the fillna function to replace empty values in price_per_unit with the price_per_unit_median value. The inplace argument is left as True so the change is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dispaly the histogram again to view the change as a large column that is over a count of 175 that is a direct result of filling in the missing values we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the describe function to check if the mean has changed and it has but only 3 units of the currency the price is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the drop_duplicates value to see drop any values that are the exact same. This is to account for any entry error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print to ensure the values we just filled are in the dataset. Since price_per_unit and units_sold return false they have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storage = df['product_sku'].fillna(\"sku_undefined\")\n",
    "df_storage = df['region'].fillna(\"region_missing\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fills the missing product_sku and missing region with text values and stores them in a dataset in df_storage. This is so if someone from the internal side of the company can look in the code fill in those missing variables and re run the dataset they can do it within the code. They would not have to input the values into the excel stored on github as they could enter them directly into df_storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks to ensure that product_sku and regions are still missing values in 'df'. This is because we do not believe they are unusable for the analysis detailed at the beginning due to the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_storage.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks that dataframe storage has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code then drops the missing values in product_sku and region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the dataframe again cause we have not seen it in awhile just to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks that dataframe 'df' has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some basic info that mainly shows us that we are now at 810 entries from the initial 1050. This means we have cleared about 240 entries from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some statistics and see that our mean for both units_sold and price_per_unit have changed slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed above that their was an outlier in the histogram and now we are going to treat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_per_unit']>2000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate for any indexes above 2000 and we find index 969."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(969, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop index 969 from the dataplace permanently with the drop function and inplace = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print our new histogram to see that we have removed the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process with units_sold to check for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['units_sold']>20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate for indexes above 20 and find index 719."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(719, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop index 969 from the dataplace permanently with the drop function and inplace = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print our new histogram to see that we have removed the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x = 'price_per_unit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print a boxplot to check if their are any more outliers in the data. We come across a lot of data being greater than 300 which approximatley the upper extreme of the whisker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_per_unit']>300]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate those under 300 and determine there are a lot of indices above 300 and decide to come back to it using a different plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x = 'units_sold')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a boxplot of the units_sold which presents a more reasonable boxplot since it is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['units_sold']>18]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate for units above the upper extreme and find some indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['units_sold']>18].index, axis=0, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use the drop function to permanently drop all indices that met the boolean of being greater than 18 using inpalce=True to make it permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x = 'units_sold')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remake the boxplot to show no more outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='units_sold',y='price_per_unit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a scatterplot to see if we can further find outliers in price_per_unit or if the boxplot was just misdjuding due the data being skewed to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x='price_per_unit',y='units_sold', fit_reg=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a scatterplot with a line of best and a different orientation to see if we can further enhance the findings from the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_per_unit']>550]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the two scatterplots and the boxplot there seems to be a cluster data from 300 to 550. However past 550 there are only a few datapoints. Therefore the conclusion is to isolate for indices greater than 550."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['price_per_unit']>550].index, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conclusion seems much more reasonable than the number of outliers concluded from the sole analysis of the boxplot. Therefore we will use the drop function to permanaently drop any index above 550 using inplace=True to ensure it is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed our outliers we can see that the units_sold mean has marginally changed from the initial 10.04 to 10.03. While the price_per_unit mean slightly decreased from the initial 107.59 to 98.03. We make a careful note that this was due to removing & filling missing data, dropping duplicates, removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x='price_per_unit',y='units_sold', fit_reg=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reprint scatter plot with line of best fit to ensure the data looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['product_sku'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.replace(['Product_46', 'Product_36', 'Product_17', 'Product_45', 'Product_31',\n",
    " 'Product_18', 'Product_16', 'Product_9', 'Product_20', 'Product_1',\n",
    " 'Product_2', 'Product_3', 'Product_43', 'Product_33', 'Product_14',\n",
    " 'Product_50', 'Product_6', 'Product_13', 'Product_11', 'Product_27',\n",
    " 'Product_28', 'Product_24', 'Product_34', 'Product_25', 'Product_4',\n",
    " 'Product_48', 'Product_10', 'Product_49', 'Product_12', 'Product_42',\n",
    " 'Product_5', 'Product_26', 'Product_8', 'Product_37', 'Product_22',\n",
    " 'Product_32', 'Product_23', 'Product_35', 'Product_15', 'Product_29',\n",
    " 'Product_44', 'Product_47', 'Product_41', 'Product_39', 'Product_30',\n",
    " 'Product_40', 'Product_21', 'Product_38', 'Product_19', 'Product_7'], ['P_46', 'P_36', 'P_17', 'P_45', 'P_31',\n",
    " 'P_18', 'P_16', 'P_9', 'P_20', 'P_1',\n",
    " 'P_2', 'P_3', 'P_43', 'P_33', 'P_14',\n",
    " 'P_50', 'P_6', 'P_13', 'P_11', 'P_27',\n",
    " 'P_28', 'P_24', 'P_34', 'P_25', 'P_4',\n",
    " 'P_48', 'P_10', 'P_49', 'P_12', 'P_42',\n",
    " 'P_5', 'P_26', 'P_8', 'P_37', 'P_22',\n",
    " 'P_32', 'P_23', 'P_35', 'P_15', 'P_29',\n",
    " 'P_44', 'P_47', 'P_41', 'P_39', 'P_30',\n",
    " 'P_40', 'P_21', 'P_38', 'P_19', 'P_7'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Region_3' 'Region_1' 'Region_6' 'Region_7' 'Region_9' 'Region_2'\n",
      " 'Region_4' 'Region_5' 'Region_8' 'Region_10']\n"
     ]
    }
   ],
   "source": [
    "print(df['region'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(['Region_3' 'Region_1' 'Region_6' 'Region_7' 'Region_9' 'Region_2'\n",
    " 'Region_4' 'Region_5' 'Region_8' 'Region_10'], ['R_3' 'R_1' 'R_6' 'R_7' 'R_9' 'R_2'\n",
    " 'R_4' 'R_5' 'R_8' 'R_10'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_sku</th>\n",
       "      <th>region</th>\n",
       "      <th>price_per_unit</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>year_sold</th>\n",
       "      <th>sales_revenue</th>\n",
       "      <th>month_sold</th>\n",
       "      <th>quarter_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_46</td>\n",
       "      <td>Region_3</td>\n",
       "      <td>20.43</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>245.160000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_36</td>\n",
       "      <td>Region_1</td>\n",
       "      <td>12.77</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>127.700000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_17</td>\n",
       "      <td>Region_6</td>\n",
       "      <td>125.69</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>754.140000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P_45</td>\n",
       "      <td>Region_1</td>\n",
       "      <td>8.63</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>94.930000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P_31</td>\n",
       "      <td>Region_3</td>\n",
       "      <td>23.73</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>142.380000</td>\n",
       "      <td>1</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>P_42</td>\n",
       "      <td>Region_9</td>\n",
       "      <td>240.97</td>\n",
       "      <td>10.048626</td>\n",
       "      <td>2024-09-15</td>\n",
       "      <td>2024</td>\n",
       "      <td>2421.417357</td>\n",
       "      <td>9</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>P_7</td>\n",
       "      <td>Region_9</td>\n",
       "      <td>128.51</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>2024</td>\n",
       "      <td>2184.670000</td>\n",
       "      <td>9</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>P_1</td>\n",
       "      <td>Region_4</td>\n",
       "      <td>95.24</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2024-09-21</td>\n",
       "      <td>2024</td>\n",
       "      <td>666.680000</td>\n",
       "      <td>9</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>P_2</td>\n",
       "      <td>Region_10</td>\n",
       "      <td>43.44</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>2024-09-25</td>\n",
       "      <td>2024</td>\n",
       "      <td>651.600000</td>\n",
       "      <td>9</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>P_35</td>\n",
       "      <td>Region_8</td>\n",
       "      <td>56.95</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>2024</td>\n",
       "      <td>341.700000</td>\n",
       "      <td>9</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_sku     region  price_per_unit  units_sold   sale_date  year_sold   \n",
       "2          P_46   Region_3           20.43   12.000000  2022-01-03       2022  \\\n",
       "3          P_36   Region_1           12.77   10.000000  2022-01-04       2022   \n",
       "4          P_17   Region_6          125.69    6.000000  2022-01-05       2022   \n",
       "5          P_45   Region_1            8.63   11.000000  2022-01-06       2022   \n",
       "6          P_31   Region_3           23.73    6.000000  2022-01-07       2022   \n",
       "..          ...        ...             ...         ...         ...        ...   \n",
       "988        P_42   Region_9          240.97   10.048626  2024-09-15       2024   \n",
       "993         P_7   Region_9          128.51   17.000000  2024-09-20       2024   \n",
       "994         P_1   Region_4           95.24    7.000000  2024-09-21       2024   \n",
       "998         P_2  Region_10           43.44   15.000000  2024-09-25       2024   \n",
       "999        P_35   Region_8           56.95    6.000000  2024-09-26       2024   \n",
       "\n",
       "     sales_revenue  month_sold quarter_sold  \n",
       "2       245.160000           1       2022Q1  \n",
       "3       127.700000           1       2022Q1  \n",
       "4       754.140000           1       2022Q1  \n",
       "5        94.930000           1       2022Q1  \n",
       "6       142.380000           1       2022Q1  \n",
       "..             ...         ...          ...  \n",
       "988    2421.417357           9       2024Q3  \n",
       "993    2184.670000           9       2024Q3  \n",
       "994     666.680000           9       2024Q3  \n",
       "998     651.600000           9       2024Q3  \n",
       "999     341.700000           9       2024Q3  \n",
       "\n",
       "[799 rows x 9 columns]"
      ]
     },
     "execution_count": 536,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year_sold'] = pd.DatetimeIndex(df['sale_date']).year\n",
    "df['month_sold'] = pd.DatetimeIndex(df['sale_date']).month\n",
    "df['sales_revenue'] = df['units_sold'] * df['price_per_unit']\n",
    "df['quarter_sold'] = pd.PeriodIndex(df.sale_date, freq='Q')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1  2  3  4  5  6  7  8  9 10 11 12]\n"
     ]
    }
   ],
   "source": [
    "print(df['month_sold'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_sku</th>\n",
       "      <th>region</th>\n",
       "      <th>price_per_unit</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>year_sold</th>\n",
       "      <th>sales_revenue</th>\n",
       "      <th>month_sold</th>\n",
       "      <th>quarter_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P_46</td>\n",
       "      <td>Region_3</td>\n",
       "      <td>20.43</td>\n",
       "      <td>Dec</td>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>2022</td>\n",
       "      <td>245.160000</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>P_36</td>\n",
       "      <td>Region_1</td>\n",
       "      <td>12.77</td>\n",
       "      <td>Oct</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>127.700000</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>P_17</td>\n",
       "      <td>Region_6</td>\n",
       "      <td>125.69</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>2022</td>\n",
       "      <td>754.140000</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>P_45</td>\n",
       "      <td>Region_1</td>\n",
       "      <td>8.63</td>\n",
       "      <td>Nov</td>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>94.930000</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>P_31</td>\n",
       "      <td>Region_3</td>\n",
       "      <td>23.73</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2022-01-07</td>\n",
       "      <td>2022</td>\n",
       "      <td>142.380000</td>\n",
       "      <td>Jan</td>\n",
       "      <td>2022Q1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>P_42</td>\n",
       "      <td>Region_9</td>\n",
       "      <td>240.97</td>\n",
       "      <td>10.048626</td>\n",
       "      <td>2024-09-15</td>\n",
       "      <td>2024</td>\n",
       "      <td>2421.417357</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>P_7</td>\n",
       "      <td>Region_9</td>\n",
       "      <td>128.51</td>\n",
       "      <td>17.0</td>\n",
       "      <td>2024-09-20</td>\n",
       "      <td>2024</td>\n",
       "      <td>2184.670000</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>P_1</td>\n",
       "      <td>Region_4</td>\n",
       "      <td>95.24</td>\n",
       "      <td>Jul</td>\n",
       "      <td>2024-09-21</td>\n",
       "      <td>2024</td>\n",
       "      <td>666.680000</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>P_2</td>\n",
       "      <td>Region_10</td>\n",
       "      <td>43.44</td>\n",
       "      <td>15.0</td>\n",
       "      <td>2024-09-25</td>\n",
       "      <td>2024</td>\n",
       "      <td>651.600000</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>P_35</td>\n",
       "      <td>Region_8</td>\n",
       "      <td>56.95</td>\n",
       "      <td>Jun</td>\n",
       "      <td>2024-09-26</td>\n",
       "      <td>2024</td>\n",
       "      <td>341.700000</td>\n",
       "      <td>Sep</td>\n",
       "      <td>2024Q3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    product_sku     region  price_per_unit units_sold   sale_date  year_sold   \n",
       "2          P_46   Region_3           20.43        Dec  2022-01-03       2022  \\\n",
       "3          P_36   Region_1           12.77        Oct  2022-01-04       2022   \n",
       "4          P_17   Region_6          125.69        Jun  2022-01-05       2022   \n",
       "5          P_45   Region_1            8.63        Nov  2022-01-06       2022   \n",
       "6          P_31   Region_3           23.73        Jun  2022-01-07       2022   \n",
       "..          ...        ...             ...        ...         ...        ...   \n",
       "988        P_42   Region_9          240.97  10.048626  2024-09-15       2024   \n",
       "993         P_7   Region_9          128.51       17.0  2024-09-20       2024   \n",
       "994         P_1   Region_4           95.24        Jul  2024-09-21       2024   \n",
       "998         P_2  Region_10           43.44       15.0  2024-09-25       2024   \n",
       "999        P_35   Region_8           56.95        Jun  2024-09-26       2024   \n",
       "\n",
       "     sales_revenue month_sold quarter_sold  \n",
       "2       245.160000        Jan       2022Q1  \n",
       "3       127.700000        Jan       2022Q1  \n",
       "4       754.140000        Jan       2022Q1  \n",
       "5        94.930000        Jan       2022Q1  \n",
       "6       142.380000        Jan       2022Q1  \n",
       "..             ...        ...          ...  \n",
       "988    2421.417357        Sep       2024Q3  \n",
       "993    2184.670000        Sep       2024Q3  \n",
       "994     666.680000        Sep       2024Q3  \n",
       "998     651.600000        Sep       2024Q3  \n",
       "999     341.700000        Sep       2024Q3  \n",
       "\n",
       "[799 rows x 9 columns]"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.replace([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12] , ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.pivot_table(index='region', columns='product_sku', values='units_sold')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = df[(df.region == 'Region_3')].pivot_table(index=['region', 'product_sku'], values=['units_sold', 'price_per_unit'],\n",
    "                                 aggfunc=np.mean)\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = df[(df.region == 'Region_3')].pivot_table(index='region', columns='product_sku', \n",
    "                                               values='sales_revenue')\n",
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= sns.barplot(data = p3)\n",
    "t.set_xlabel(\"X Label\",fontsize=2)\n",
    "t.tick_params(labelsize=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
