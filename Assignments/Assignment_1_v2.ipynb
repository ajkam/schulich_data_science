{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective for Data Analysis: I will be assuming the role of a data analyst in an e-commerce company that operates overall multiple regions with multiple product skews. The goal of this data analysis is to understand more about the sales performance of various products.\n",
    "The data analysis will take three steps:\n",
    "Step 1: Cleaning the data.\n",
    "\tThis will involve removing missing values and outliers.\n",
    "Step 2: Transforming the data.\n",
    "\tThis will involve adding new columns and modifying data to make it easier for analysis.\n",
    "Step 3: Analyzing the data.\n",
    "\tThis will involve analyzing the data at an overall VP level and analyzing the data at a regional sales manager level.\n",
    "Each step will have its own description before it starts. Furthermore, each line of code will be annotated to explain its purpose. Also any assumptions that are made will be stated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import scipy.stats"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is necessary to import pandas, seaborn, numpy, matplotlib, math, and scipy numpy which are the packages required for this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/ajkam/schulich_data_science/main/Assignments/sales_data%20(1)%20-%20Copy.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above line of code is used to read the excel file store on github and store into a dataframe titled 'df'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display info on the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe some statistics regarding the datrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Cleaning the data.\n",
    "\n",
    "The data analysis will help the sales managers and sales VP come up with an efficient strategy for the 2025 sales year. Before coming up with those questions to answer, visualizing observations, and recommending actions, I will start by cleaning the dataset. \n",
    "\n",
    "The idea of cleaning this dataset is to ensure that someone coming into this code for the first time will be able to understand precisely what variables I removed and why I did so. I would also like the guarantee that if they need to make modifications to my analysis due to new information on values or rerun the same analysis for themselves, they will have the means to do so within the code.\n",
    "\n",
    "The issue of tackling missing data will be solved by deciding to replace and fill in the values.\n",
    "If the missing data is numerical then the data should be filled with an appropriate value-based central limit theory. This is because it will not affect the spread of the data. \n",
    "\n",
    "If the missing data is categorical, then the data should be removed for the purpose of the analysis. This is because filling these values with an appropriate value based on the central limit theory would affect the spread of data.\n",
    "\n",
    "The next part would be to take care of outliers. To identify if there are outliers histograms, boxplots, and scatterplots will be used to determine if the data has outliers. Then based on central limit theory they will be removed from the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_dict = {'Product':'product_sku', 'Region':'region','Price':'price_per_unit', \n",
    "               'Units Sold':'units_sold','Sale Date':'sale_date' }\n",
    "df= df.rename(columns=column_dict)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of  the old column headings and new easier to work with headings\n",
    "use the rename function to replace the column headings. Note that an assumption was made that the Price column represented price per unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the isna function to check if their any missing values in each column and sum the total number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the boolean function .any to check if the dataframe columns has any missing values so its easy to compare with down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the data set with the boolean function isnull to see true and false for where data is missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram for the units_sold column to see how it is distributed. Based on the output it seems that is normally distributed with no skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data =df['units_sold']\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temporary dataframe called data with the units sold column to check kurtosis and skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kurtosis (scipy): \", scipy.stats.kurtosis(data, fisher=True))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the scipy.stats.kurtosis to check the kurtosis value.\n",
    "Since it is positive yet very close to zero the data is very slightly peaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness(data):\n",
    "    n = len(data)\n",
    "    mean = sum(data) / n\n",
    "    variance = sum((x - mean) ** 2 for x in data) / n\n",
    "    std_dev = math.sqrt(variance)\n",
    "    \n",
    "    skewness = sum((x - mean) ** 3 for x in data) / (n * std_dev ** 3)\n",
    "    return skewness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a formula using the length of the data, the mean of the data divided by length, the variance which is calculated by getting the total sum of squares, and then the standard deviation which is the square root of variance.\n",
    "\n",
    "The calculate the skewneness which is sum each datapoint subratec the mean cubed divied by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skewness(data))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above return the skewness. Since the skew is between -0.5 and 0.5 that data is not skewed positively or negatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "units_sold_mean = df['units_sold'].mean()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it is normally distriubted with no skewness I have determined the best approach is to fill the missing data with the mean. So this function uses the .mean() function to get the mean of units sold and store the value in units_sold_mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['units_sold'].fillna(value=units_sold_mean, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the fillna function to replace empty values in units_sold with the units_sold_mean value. The inplace argument is left as True so the change is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dispaly the histogram again to view the change as a new column that is over a count of 200 is a direct result of filling in the missing values we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the describe function to check if the mean has changed and it has not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a histogram for the price_per_unit column to see how it is distributed. Based on the output it seems that the data skewed to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 =df['price_per_unit']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a temporary dataframe called data2 with the price_per_unit column to check kurtosis and skewness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kurtosis (scipy): \", scipy.stats.kurtosis(data2, fisher=True))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the scipy.stats.kurtosis to check the kurtosis value.\n",
    "Since it is positive and greater than 3 this indicates a leptokurtic distriubtion. This means the data has very thick tails, with a high concentration of data around the tails, and there is a high chance of outliers. This means we should be careful when removing outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skewness(data2):\n",
    "    n = len(data2)\n",
    "    mean = sum(data2) / n\n",
    "    variance = sum((x - mean) ** 2 for x in data2) / n\n",
    "    std_dev = math.sqrt(variance)\n",
    "    \n",
    "    skewness = sum((x - mean) ** 3 for x in data2) / (n * std_dev ** 3)\n",
    "    return skewness"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a formula using the length of the data, the mean of the data divided by length, the variance which is calculated by getting the total sum of squares, and then the standard deviation which is the square root of variance.\n",
    "\n",
    "The calculate the skewneness which is sum each datapoint subratec the mean cubed divied by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skewness(data2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above return the skewness. Since the skew is greater than 1 the data is extremely positivley skewed. This means that there is a huge tail on the right hand side of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_per_unit_median = df['price_per_unit'].median()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there the data is extremely positively skewed I have determined the best approach is to fill the missing data with the median. So this function uses the .median() function to get the median of price_per_unit and store the value in price_per_unit_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_per_unit'].fillna(value=price_per_unit_median, inplace= True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the fillna function to replace empty values in price_per_unit with the price_per_unit_median value. The inplace argument is left as True so the change is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dispaly the histogram again to view the change as a large column that is over a count of 175 that is a direct result of filling in the missing values we saw earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the describe function to check if the mean has changed and it has but only 3 units of the currency the price is in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the drop_duplicates value to see drop any values that are the exact same. This is to account for any entry error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print to ensure the values we just filled are in the dataset. Since price_per_unit and units_sold return false they have been filled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_storage = df['product_sku'].fillna(\"sku_undefined\")\n",
    "df_storage = df['region'].fillna(\"region_missing\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code fills the missing product_sku and missing region with text values and stores them in a dataset in df_storage. This is so if someone from the internal side of the company can look in the code fill in those missing variables and re run the dataset they can do it within the code. They would not have to input the values into the excel stored on github as they could enter them directly into df_storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks to ensure that product_sku and regions are still missing values in 'df'. This is because we do not believe they are unusable for the analysis detailed at the beginning due to the missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_storage.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks that dataframe storage has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code then drops the missing values in product_sku and region. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print the dataframe again cause we have not seen it in awhile just to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isna().any())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code checks that dataframe 'df' has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some basic info that mainly shows us that we are now at 810 entries from the initial 1050. This means we have cleared about 240 entries from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print some statistics and see that our mean for both units_sold and price_per_unit have changed slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We noticed above that their was an outlier in the histogram and now we are going to treat it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_per_unit']>2000]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate for any indexes above 2000 and we find index 969."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(969, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop index 969 from the dataplace permanently with the drop function and inplace = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['price_per_unit'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print our new histogram to see that we have removed the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the same process with units_sold to check for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['units_sold']>20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate for indexes above 20 and find index 719."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(719, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We drop index 969 from the dataplace permanently with the drop function and inplace = True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df['units_sold'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print our new histogram to see that we have removed the outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x = 'price_per_unit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We print a boxplot to check if their are any more outliers in the data. We come across a lot of data being greater than 300 which approximatley the upper extreme of the whisker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_per_unit']>300]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate those under 300 and determine there are a lot of indices above 300 and decide to come back to analyze it further using a different plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x = 'units_sold')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a boxplot of the units_sold which presents a more reasonable boxplot since it is normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['units_sold']>18]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We isolate for units above the upper extreme and find some indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['units_sold']>18].index, axis=0, inplace=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decide to use the drop function to permanently drop all indices that met the boolean of being greater than 18 using inpalce=True to make it permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data = df, x = 'units_sold')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remake the boxplot to show no more outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data=df, x='units_sold',y='price_per_unit')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a scatterplot to see if we can further find outliers in price_per_unit or if the boxplot was just misdjuding due the data being skewed to the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x='price_per_unit',y='units_sold', fit_reg=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a scatterplot with a line of best and a different orientation to see if we can further enhance the findings from the scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['price_per_unit']>550]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the two scatterplots and the boxplot there seems to be a cluster data from 300 to 550. However past 550 there are only a few datapoints. Therefore the conclusion is to isolate for indices greater than 550."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df[df['price_per_unit']>550].index, axis=0, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This conclusion seems much more reasonable than the number of outliers concluded from the sole analysis of the boxplot. Therefore we will use the drop function to permanaently drop any index above 550 using inplace=True to ensure it is permanent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(data=df, x='price_per_unit',y='units_sold', fit_reg=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We reprint scatter plot with line of best fit to ensure the data looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the info function to determine that we now have 799 entries. This means we have cleaned about 11 entries from the 810. In total we have cleaned 11+240=251 entries from the orignal data set of 1050 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed our outliers we can see that the units_sold mean has marginally changed from the initial 10.04 to 10.03. While the price_per_unit mean slightly decreased from the initial 107.59 to 98.03. We make a careful note that this was due to removing & filling missing data, dropping duplicates, removing outliers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 and 3:\n",
    "I will transform the data based on the following questions I want to answer. Then I will use the data to visualize the observations. These observations will take two categories. Firstly, the data of summary statistics will be organized and visualized to figure out the big-picture decisions on a department-wide and regional sales level. Secondly, specific product-by-region recommendations will be made according to the recommendation matrix to find the specific fixes for the sales performance of products on department-wide and regional sales levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['product_sku'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly I want to prepare the data so that the labels are shorter and taht they can be sorted numerically so I am going to print a unique list of product_sku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.replace(['Product_46', 'Product_36', 'Product_17', 'Product_45', 'Product_31',\n",
    " 'Product_18', 'Product_16', 'Product_9', 'Product_20', 'Product_1',\n",
    " 'Product_2', 'Product_3', 'Product_43', 'Product_33', 'Product_14',\n",
    " 'Product_50', 'Product_6', 'Product_13', 'Product_11', 'Product_27',\n",
    " 'Product_28', 'Product_24', 'Product_34', 'Product_25', 'Product_4',\n",
    " 'Product_48', 'Product_10', 'Product_49', 'Product_12', 'Product_42',\n",
    " 'Product_5', 'Product_26', 'Product_8', 'Product_37', 'Product_22',\n",
    " 'Product_32', 'Product_23', 'Product_35', 'Product_15', 'Product_29',\n",
    " 'Product_44', 'Product_47', 'Product_41', 'Product_39', 'Product_30',\n",
    " 'Product_40', 'Product_21', 'Product_38', 'Product_19', 'Product_7'], [46, 36, 17, 45, 31,\n",
    " 18, 16, 9, 20, 1,\n",
    " 2, 3, 43, 33, 14,\n",
    " 50, 6, 13, 11, 27,\n",
    " 28, 24, 34, 25, 4,\n",
    " 48, 10, 49, 12, 42,\n",
    " 5, 26, 8, 37, 22,\n",
    " 32, 23, 35, 15, 29,\n",
    " 44, 47, 41, 39, 30,\n",
    " 40, 21, 38, 19, 7]\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I am going to replace the string Product_#X with the integer #X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['product_sku'], ascending=True,inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I am going to sort by product_sku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['region'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am going to do the same with the labels for Region starting printing a list of unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace(['Region_3', 'Region_1', 'Region_6', 'Region_7', 'Region_9', 'Region_2',\n",
    " 'Region_4', 'Region_5', 'Region_8', 'Region_10'], [3, 1, 6, 7, 9, 2,\n",
    " 4, 5, 8, 10] \n",
    ")\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am replacing the string Region_#X with the integer #X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=['region'], ascending=True,inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I am going to sort by region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_titles = [\"region\",\"product_sku\",\"price_per_unit\",\"units_sold\",\"sale_date\"]\n",
    "df=df.reindex(columns=columns_titles)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided that for my analysis it would better to have region first so I reindexed the columns accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.groupby('product_sku')['units_sold'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>region</th>\n",
       "      <th>product_sku</th>\n",
       "      <th>price_per_unit</th>\n",
       "      <th>units_sold</th>\n",
       "      <th>sale_date</th>\n",
       "      <th>year_sold</th>\n",
       "      <th>month_sold</th>\n",
       "      <th>quarter_sold</th>\n",
       "      <th>sales_revenue</th>\n",
       "      <th>CW_units_sold</th>\n",
       "      <th>CW_price_per_unit</th>\n",
       "      <th>CW_sales_revenue</th>\n",
       "      <th>Dif_sales_revenue</th>\n",
       "      <th>Dif_price_per_unit</th>\n",
       "      <th>Dif_units_sold</th>\n",
       "      <th>mtr_sales_revenue</th>\n",
       "      <th>mtr_price_per_unit</th>\n",
       "      <th>mtr_units_sold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>758</th>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>78.13</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2024-01-29</td>\n",
       "      <td>2024</td>\n",
       "      <td>1</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>937.56</td>\n",
       "      <td>9.224706</td>\n",
       "      <td>109.221429</td>\n",
       "      <td>1032.719496</td>\n",
       "      <td>-0.092145</td>\n",
       "      <td>-0.284664</td>\n",
       "      <td>0.300855</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>17.00</td>\n",
       "      <td>16.0</td>\n",
       "      <td>2022-10-23</td>\n",
       "      <td>2022</td>\n",
       "      <td>10</td>\n",
       "      <td>2022Q4</td>\n",
       "      <td>272.00</td>\n",
       "      <td>10.178141</td>\n",
       "      <td>90.996087</td>\n",
       "      <td>820.796576</td>\n",
       "      <td>-0.668615</td>\n",
       "      <td>-0.813179</td>\n",
       "      <td>0.571996</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>12.77</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>2022Q1</td>\n",
       "      <td>127.70</td>\n",
       "      <td>9.008104</td>\n",
       "      <td>82.728333</td>\n",
       "      <td>816.042056</td>\n",
       "      <td>-0.843513</td>\n",
       "      <td>-0.845639</td>\n",
       "      <td>0.110111</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>56.04</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2022-04-24</td>\n",
       "      <td>2022</td>\n",
       "      <td>4</td>\n",
       "      <td>2022Q2</td>\n",
       "      <td>560.40</td>\n",
       "      <td>10.178141</td>\n",
       "      <td>90.996087</td>\n",
       "      <td>820.796576</td>\n",
       "      <td>-0.317249</td>\n",
       "      <td>-0.384149</td>\n",
       "      <td>-0.017502</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>26.20</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2023-12-15</td>\n",
       "      <td>2023</td>\n",
       "      <td>12</td>\n",
       "      <td>2023Q4</td>\n",
       "      <td>235.80</td>\n",
       "      <td>8.899695</td>\n",
       "      <td>73.858148</td>\n",
       "      <td>614.888196</td>\n",
       "      <td>-0.616516</td>\n",
       "      <td>-0.645266</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>143.74</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2022-09-06</td>\n",
       "      <td>2022</td>\n",
       "      <td>9</td>\n",
       "      <td>2022Q3</td>\n",
       "      <td>1581.14</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>119.125500</td>\n",
       "      <td>1204.174500</td>\n",
       "      <td>0.313049</td>\n",
       "      <td>0.206627</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>102.84</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2023-03-17</td>\n",
       "      <td>2023</td>\n",
       "      <td>3</td>\n",
       "      <td>2023Q1</td>\n",
       "      <td>1336.92</td>\n",
       "      <td>9.619215</td>\n",
       "      <td>92.478333</td>\n",
       "      <td>847.491646</td>\n",
       "      <td>0.577502</td>\n",
       "      <td>0.112044</td>\n",
       "      <td>0.351462</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>793</th>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>71.78</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2024-03-04</td>\n",
       "      <td>2024</td>\n",
       "      <td>3</td>\n",
       "      <td>2024Q1</td>\n",
       "      <td>933.14</td>\n",
       "      <td>10.478506</td>\n",
       "      <td>66.860952</td>\n",
       "      <td>679.324396</td>\n",
       "      <td>0.373629</td>\n",
       "      <td>0.073571</td>\n",
       "      <td>0.240635</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>133.35</td>\n",
       "      <td>14.0</td>\n",
       "      <td>2022-12-18</td>\n",
       "      <td>2022</td>\n",
       "      <td>12</td>\n",
       "      <td>2022Q4</td>\n",
       "      <td>1866.90</td>\n",
       "      <td>10.721232</td>\n",
       "      <td>130.055714</td>\n",
       "      <td>1498.127976</td>\n",
       "      <td>0.246155</td>\n",
       "      <td>0.025330</td>\n",
       "      <td>0.305820</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>10</td>\n",
       "      <td>50</td>\n",
       "      <td>60.19</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2024-05-12</td>\n",
       "      <td>2024</td>\n",
       "      <td>5</td>\n",
       "      <td>2024Q2</td>\n",
       "      <td>782.47</td>\n",
       "      <td>10.954863</td>\n",
       "      <td>109.572000</td>\n",
       "      <td>1195.173263</td>\n",
       "      <td>-0.345308</td>\n",
       "      <td>-0.450681</td>\n",
       "      <td>0.186688</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>High</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>799 rows Ã— 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     region  product_sku  price_per_unit  units_sold   sale_date  year_sold   \n",
       "758       1           22           78.13        12.0  2024-01-29       2024  \\\n",
       "295       1           13           17.00        16.0  2022-10-23       2022   \n",
       "3         1           36           12.77        10.0  2022-01-04       2022   \n",
       "113       1           13           56.04        10.0  2022-04-24       2022   \n",
       "713       1            5           26.20         9.0  2023-12-15       2023   \n",
       "..      ...          ...             ...         ...         ...        ...   \n",
       "248      10           31          143.74        11.0  2022-09-06       2022   \n",
       "440      10           44          102.84        13.0  2023-03-17       2023   \n",
       "793      10           43           71.78        13.0  2024-03-04       2024   \n",
       "351      10           14          133.35        14.0  2022-12-18       2022   \n",
       "862      10           50           60.19        13.0  2024-05-12       2024   \n",
       "\n",
       "     month_sold quarter_sold  sales_revenue  CW_units_sold  CW_price_per_unit   \n",
       "758           1       2024Q1         937.56       9.224706         109.221429  \\\n",
       "295          10       2022Q4         272.00      10.178141          90.996087   \n",
       "3             1       2022Q1         127.70       9.008104          82.728333   \n",
       "113           4       2022Q2         560.40      10.178141          90.996087   \n",
       "713          12       2023Q4         235.80       8.899695          73.858148   \n",
       "..          ...          ...            ...            ...                ...   \n",
       "248           9       2022Q3        1581.14      10.000000         119.125500   \n",
       "440           3       2023Q1        1336.92       9.619215          92.478333   \n",
       "793           3       2024Q1         933.14      10.478506          66.860952   \n",
       "351          12       2022Q4        1866.90      10.721232         130.055714   \n",
       "862           5       2024Q2         782.47      10.954863         109.572000   \n",
       "\n",
       "     CW_sales_revenue  Dif_sales_revenue  Dif_price_per_unit  Dif_units_sold   \n",
       "758       1032.719496          -0.092145           -0.284664        0.300855  \\\n",
       "295        820.796576          -0.668615           -0.813179        0.571996   \n",
       "3          816.042056          -0.843513           -0.845639        0.110111   \n",
       "113        820.796576          -0.317249           -0.384149       -0.017502   \n",
       "713        614.888196          -0.616516           -0.645266        0.011271   \n",
       "..                ...                ...                 ...             ...   \n",
       "248       1204.174500           0.313049            0.206627        0.100000   \n",
       "440        847.491646           0.577502            0.112044        0.351462   \n",
       "793        679.324396           0.373629            0.073571        0.240635   \n",
       "351       1498.127976           0.246155            0.025330        0.305820   \n",
       "862       1195.173263          -0.345308           -0.450681        0.186688   \n",
       "\n",
       "    mtr_sales_revenue mtr_price_per_unit mtr_units_sold  \n",
       "758               Low                Low           High  \n",
       "295               Low                Low           High  \n",
       "3                 Low                Low           High  \n",
       "113               Low                Low            Low  \n",
       "713               Low                Low           High  \n",
       "..                ...                ...            ...  \n",
       "248              High               High           High  \n",
       "440              High               High           High  \n",
       "793              High               High           High  \n",
       "351              High               High           High  \n",
       "862               Low                Low           High  \n",
       "\n",
       "[799 rows x 18 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['year_sold'] = pd.DatetimeIndex(df['sale_date']).year\n",
    "df['month_sold'] = pd.DatetimeIndex(df['sale_date']).month\n",
    "df['quarter_sold'] = pd.PeriodIndex(df.sale_date, freq='Q')\n",
    "df['sales_revenue'] = df['units_sold'] * df['price_per_unit']\n",
    "df['CW_units_sold'] = df.groupby('product_sku')['units_sold'].transform('mean')\n",
    "df['CW_price_per_unit'] = df.groupby('product_sku')['price_per_unit'].transform('mean')\n",
    "df['CW_sales_revenue'] = df.groupby('product_sku')['sales_revenue'].transform('mean')\n",
    "df['Dif_sales_revenue'] = (df['sales_revenue']- df['CW_sales_revenue'])/df['CW_sales_revenue']\n",
    "df['Dif_price_per_unit'] = (df['price_per_unit']- df['CW_price_per_unit'])/df['CW_price_per_unit']\n",
    "df['Dif_units_sold'] = (df['units_sold']- df['CW_units_sold'])/df['CW_units_sold']\n",
    "\n",
    "\n",
    "\n",
    "def conditions(df):    \n",
    "    if df['Dif_sales_revenue'] > 0: matrix = \"High\"\n",
    "    elif df['Dif_sales_revenue'] == 0:     matrix = \"Avg\"\n",
    "    elif df['Dif_sales_revenue'] < 0:     matrix = \"Low\"\n",
    "    else: matrix = \"ERR\"\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "df['mtr_sales_revenue'] = df.apply(conditions, axis=1)\n",
    "\n",
    "\n",
    "def conditions1(df):    \n",
    "    if df['Dif_price_per_unit'] > 0: matrix = \"High\"\n",
    "    elif df['Dif_price_per_unit'] == 0:     matrix = \"Avg\"\n",
    "    elif df['Dif_price_per_unit'] < 0:     matrix = \"Low\"\n",
    "    else: matrix = \"ERR\"\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "df['mtr_price_per_unit'] = df.apply(conditions1, axis=1)\n",
    "\n",
    "def conditions2(df):    \n",
    "    if df['Dif_units_sold'] > 0: matrix = \"High\"\n",
    "    elif df['Dif_units_sold'] == 0:     matrix = \"Avg\"\n",
    "    elif df['Dif_units_sold'] < 0:     matrix = \"Low\"\n",
    "    else: matrix = \"ERR\"\n",
    "        \n",
    "    return matrix\n",
    "\n",
    "df['mtr_units_sold'] = df.apply(conditions2, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above columns will help me answer my questions in the big picture statistical summary and the detailed product by region recommendation.\n",
    "The year_sold, month_sold, quarter_sold will help me output summary statistics grouped by a certain time period (year, month, and season/quarter).\n",
    "The columns with the prefix CW are the company-wide averages of units_sold, price_per_unit, and sales_revenue.\n",
    "The columns with the prefix Dif are the differential statistics based on much greater or less the units_sold, price_per_unit, and sales_revenue are to the CW versions in decimals.\n",
    "The columns with the prefix mtr are the boolean operators to determine which scenario each sale falls into based on units_sold, sales_revenue, and price_per_unit. To create these columns I created a variable called conditions. Then I stated one condition if the differential is greater than zero to return the value \"High\", if equal to zero then return the value \"Avg\", and if less than zero to return the value \"low\" in the next condition. as specified in the matrix table attached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.pivot_table(index='product_sku', columns='region', values=['units_sold'])\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.iloc[:,4].isna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2 = df[(df.region == 'R3')].pivot_table(index=['region', 'product_sku'], values=['units_sold', 'price_per_unit'],\n",
    "                                 aggfunc=np.mean)\n",
    "p2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p3 = df[(df.region == 'R3')].pivot_table(index='product_sku', columns='region', \n",
    "                                               values='sales_revenue')\n",
    "p3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= sns.barplot(data = p3, x= 'product_sku')\n",
    "t.set_xlabel(\"X Label\",fontsize=2)\n",
    "t.tick_params(labelsize=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
